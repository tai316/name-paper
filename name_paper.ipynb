{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cq6s_9VA0KBveVQja17rXWaueuS-M_aa",
      "authorship_tag": "ABX9TyMW2Gh9D3c3tPa3XP2b8CFk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tai316/name-paper/blob/main/name_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# High-accuracy PDF renamer:\n",
        "# text -> OCR -> DOI/arXiv -> Crossref/arXiv -> Title search -> rename\n",
        "# =========================\n",
        "\n",
        "# --- install deps ---\n",
        "!pip -q install pymupdf requests pandas rapidfuzz pillow pytesseract\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y tesseract-ocr\n",
        "# 日本語OCRもやるなら（必要な場合のみ。少し重い）\n",
        "!apt-get -qq install -y tesseract-ocr-jpn tesseract-ocr-jpn-vert\n",
        "\n",
        "import re\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import fitz  # PyMuPDF\n",
        "import requests\n",
        "import pandas as pd\n",
        "from rapidfuzz import fuzz\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import io\n",
        "\n",
        "# ================\n",
        "# Settings\n",
        "# ================\n",
        "TARGET_DIR = \"論文が入っているフォルダ名のパスを書く\"  # ★あなたのPDFフォルダ\n",
        "DRY_RUN = False  # True=プレビューのみ / False=実行\n",
        "RECURSIVE = True  # サブフォルダも見る\n",
        "\n",
        "PAGES_TEXT_SCAN = 2       # まずテキスト抽出で見るページ数\n",
        "PAGES_OCR_SCAN  = 2       # OCRするページ数（重いので2くらいでOK）\n",
        "\n",
        "MAX_TITLE_LEN  = 90\n",
        "MAX_AUTHOR_LEN = 40\n",
        "\n",
        "# Crossrefはmailto入りUser-Agentが推奨（適当でOK）\n",
        "CROSSREF_MAILTO = \"your_email@example.com\"\n",
        "USER_AGENT = f\"pdf-renamer/2.0 (mailto:{CROSSREF_MAILTO})\"\n",
        "\n",
        "# Crossref逆引きの一致条件（高めにして誤爆を減らす）\n",
        "MIN_TITLE_SIMILARITY = 90  # 0-100\n",
        "\n",
        "# OCR言語：英語論文中心なら \"eng\"\n",
        "# 日本語PDFも多いなら \"eng+jpn\"\n",
        "OCR_LANG = \"eng+jpn\"\n",
        "\n",
        "# ================\n",
        "# Helpers\n",
        "# ================\n",
        "def norm(s: str) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def safe_component(s: str, max_len: int) -> str:\n",
        "    s = norm(s)\n",
        "    if not s:\n",
        "        return \"UNKNOWN\"\n",
        "    s = re.sub(r'[\\\\/:*?\"<>|]', \"\", s)  # windows-safe\n",
        "    s = s.strip(\" .\")\n",
        "    s = s.replace(\" \", \"_\")\n",
        "    s = re.sub(r\"_+\", \"_\", s)\n",
        "    if len(s) > max_len:\n",
        "        s = s[:max_len].rstrip(\"_\")\n",
        "    return s or \"UNKNOWN\"\n",
        "\n",
        "def unique_path(path: Path) -> Path:\n",
        "    if not path.exists():\n",
        "        return path\n",
        "    stem, suf = path.stem, path.suffix\n",
        "    i = 2\n",
        "    while True:\n",
        "        cand = path.with_name(f\"{stem}_v{i}{suf}\")\n",
        "        if not cand.exists():\n",
        "            return cand\n",
        "        i += 1\n",
        "\n",
        "# DOI / arXiv patterns\n",
        "DOI_RE = re.compile(r\"\\b(10\\.\\d{4,9}/[^\\s\\\"<>]+)\\b\", re.IGNORECASE)\n",
        "ARXIV_RE = re.compile(r\"\\b(arXiv:\\s*)?(\\d{4}\\.\\d{4,5})(v\\d+)?\\b\", re.IGNORECASE)\n",
        "\n",
        "def find_doi(text: str) -> str | None:\n",
        "    t = text.replace(\"\\n\", \" \")\n",
        "    m = DOI_RE.search(t)\n",
        "    if not m:\n",
        "        return None\n",
        "    doi = m.group(1).rstrip(\".,);]\")\n",
        "    return doi\n",
        "\n",
        "def find_arxiv_id(text: str) -> str | None:\n",
        "    t = text.replace(\"\\n\", \" \")\n",
        "    m = ARXIV_RE.search(t)\n",
        "    return m.group(2) if m else None\n",
        "\n",
        "# ================\n",
        "# PDF reading (PyMuPDF)\n",
        "# ================\n",
        "def extract_text_pages(pdf_path: Path, pages: int) -> str:\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        out = []\n",
        "        for i in range(min(pages, doc.page_count)):\n",
        "            out.append(doc.load_page(i).get_text(\"text\"))\n",
        "        doc.close()\n",
        "        return \"\\n\".join(out)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def ocr_pages(pdf_path: Path, pages: int, dpi: int = 200) -> str:\n",
        "    \"\"\"\n",
        "    PyMuPDFでページを画像化 -> pytesseractでOCR\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        out = []\n",
        "        for i in range(min(pages, doc.page_count)):\n",
        "            page = doc.load_page(i)\n",
        "            mat = fitz.Matrix(dpi/72, dpi/72)\n",
        "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "            img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
        "            txt = pytesseract.image_to_string(img, lang=OCR_LANG)\n",
        "            out.append(txt)\n",
        "        doc.close()\n",
        "        return \"\\n\".join(out)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def extract_title_author_by_layout(pdf_path: Path) -> tuple[str | None, str | None]:\n",
        "    \"\"\"\n",
        "    フォントサイズ情報を使って “タイトルっぽい行” を拾う（テキスト抽出より安定）\n",
        "    - 1ページ目の上半分で最大フォントの行をタイトル候補\n",
        "    - 次に大きいフォント帯の行を著者候補\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        page = doc.load_page(0)\n",
        "        d = page.get_text(\"dict\")\n",
        "        h = page.rect.height\n",
        "\n",
        "        lines = []\n",
        "        for block in d.get(\"blocks\", []):\n",
        "            if block.get(\"type\") != 0:\n",
        "                continue\n",
        "            for line in block.get(\"lines\", []):\n",
        "                spans = line.get(\"spans\", [])\n",
        "                if not spans:\n",
        "                    continue\n",
        "                text = norm(\"\".join(s.get(\"text\",\"\") for s in spans))\n",
        "                if not text or len(text) < 6:\n",
        "                    continue\n",
        "                # 平均フォントサイズ\n",
        "                sizes = [s.get(\"size\", 0) for s in spans if s.get(\"size\", 0) > 0]\n",
        "                avg_size = sum(sizes)/len(sizes) if sizes else 0\n",
        "                # 行のy座標（上ほど小さい）\n",
        "                y0 = line.get(\"bbox\", [0,0,0,0])[1]\n",
        "                lines.append((y0, avg_size, text))\n",
        "\n",
        "        doc.close()\n",
        "        if not lines:\n",
        "            return None, None\n",
        "\n",
        "        # 上半分中心にタイトル探索（abstract等を避ける）\n",
        "        candidates = [(y,s,t) for (y,s,t) in lines if y < h*0.55]\n",
        "        def is_noise(t: str) -> bool:\n",
        "            low = t.lower()\n",
        "            return (\"abstract\" in low) or (\"introduction\" in low) or low.startswith(\"received\") or (\"copyright\" in low) or (\"©\" in t)\n",
        "\n",
        "        candidates = [(y,s,t) for (y,s,t) in candidates if not is_noise(t) and len(t) <= 180]\n",
        "        if not candidates:\n",
        "            return None, None\n",
        "\n",
        "        # タイトル：フォントサイズ最大の行（同率なら上に近い）\n",
        "        candidates.sort(key=lambda x: (-x[1], x[0]))\n",
        "        title = candidates[0][2]\n",
        "\n",
        "        # 著者：タイトル直後〜少し下で、メール/所属っぽくないもの\n",
        "        title_y = candidates[0][0]\n",
        "        near = [(y,s,t) for (y,s,t) in lines if title_y < y < title_y + h*0.20]\n",
        "        author = None\n",
        "        for (y,s,t) in sorted(near, key=lambda x: (x[0], -x[1])):\n",
        "            if \"@\" in t:\n",
        "                continue\n",
        "            low = t.lower()\n",
        "            if re.search(r\"\\b(university|institute|department|laboratory|school)\\b\", low):\n",
        "                continue\n",
        "            if len(t) > 140:\n",
        "                continue\n",
        "            # 人名っぽい区切り\n",
        "            if (\",\" in t) or (\" and \" in low) or (\"・\" in t) or (\"、\" in t):\n",
        "                author = re.split(r\",| and |・|、\", t)[0].strip()\n",
        "                break\n",
        "\n",
        "        return title, author\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "# ================\n",
        "# Crossref / arXiv\n",
        "# ================\n",
        "def crossref_lookup_by_doi(doi: str) -> dict | None:\n",
        "    url = f\"https://api.crossref.org/works/{doi}\"\n",
        "    try:\n",
        "        r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        return r.json().get(\"message\", None)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def pick_yyyymm_crossref(msg: dict) -> str | None:\n",
        "    for k in [\"published-print\", \"published-online\", \"issued\", \"created\"]:\n",
        "        v = msg.get(k, {})\n",
        "        parts = v.get(\"date-parts\", [])\n",
        "        if parts and parts[0]:\n",
        "            yyyy = parts[0][0]\n",
        "            mm = parts[0][1] if len(parts[0]) >= 2 else 1\n",
        "            return f\"{int(yyyy):04d}{int(mm):02d}\"\n",
        "    return None\n",
        "\n",
        "def pick_title_crossref(msg: dict) -> str | None:\n",
        "    titles = msg.get(\"title\") or []\n",
        "    return norm(titles[0]) if titles else None\n",
        "\n",
        "def pick_first_author_crossref(msg: dict) -> str | None:\n",
        "    authors = msg.get(\"author\") or []\n",
        "    if not authors:\n",
        "        return None\n",
        "    a0 = authors[0]\n",
        "    given = a0.get(\"given\", \"\") or \"\"\n",
        "    family = a0.get(\"family\", \"\") or \"\"\n",
        "    name = norm((given + \" \" + family).strip())\n",
        "    return name or None\n",
        "\n",
        "def crossref_search_by_title(title: str, author: str | None = None, rows: int = 5) -> dict | None:\n",
        "    \"\"\"\n",
        "    タイトル（+可能なら著者）でCrossref検索して、最も一致するものを返す\n",
        "    \"\"\"\n",
        "    title_q = norm(title)\n",
        "    if not title_q or len(title_q) < 8:\n",
        "        return None\n",
        "\n",
        "    params = {\n",
        "        \"query.title\": title_q,\n",
        "        \"rows\": rows,\n",
        "    }\n",
        "    if author and author != \"UNKNOWN\":\n",
        "        params[\"query.author\"] = norm(author)\n",
        "\n",
        "    try:\n",
        "        r = requests.get(\"https://api.crossref.org/works\", params=params,\n",
        "                         headers={\"User-Agent\": USER_AGENT}, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        items = r.json().get(\"message\", {}).get(\"items\", []) or []\n",
        "        if not items:\n",
        "            return None\n",
        "\n",
        "        # 最高一致のものを採用（誤爆防止でタイトル類似度チェック）\n",
        "        best = None\n",
        "        best_sim = -1\n",
        "        for it in items:\n",
        "            t = pick_title_crossref(it) or \"\"\n",
        "            sim = fuzz.token_set_ratio(title_q.lower(), t.lower())\n",
        "            if sim > best_sim:\n",
        "                best_sim = sim\n",
        "                best = it\n",
        "\n",
        "        if best and best_sim >= MIN_TITLE_SIMILARITY:\n",
        "            best[\"_title_similarity\"] = best_sim\n",
        "            return best\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def arxiv_lookup(arxiv_id: str) -> dict | None:\n",
        "    url = f\"http://export.arxiv.org/api/query?id_list={arxiv_id}\"\n",
        "    try:\n",
        "        r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        xml = r.text\n",
        "        entry_title_m = re.search(r\"<entry>.*?<title>(.*?)</title>\", xml, re.DOTALL)\n",
        "        title = norm((entry_title_m.group(1) if entry_title_m else \"\"))\n",
        "        authors = re.findall(r\"<name>(.*?)</name>\", xml)\n",
        "        pub_m = re.search(r\"<published>(\\d{4})-(\\d{2})-(\\d{2})\", xml)\n",
        "        yyyymm = f\"{pub_m.group(1)}{pub_m.group(2)}\" if pub_m else None\n",
        "        return {\"title\": title, \"authors\": authors, \"yyyymm\": yyyymm}\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ================\n",
        "# Main\n",
        "# ================\n",
        "target = Path(TARGET_DIR)\n",
        "assert target.exists(), f\"Folder not found: {target}\"\n",
        "\n",
        "pdfs = sorted(target.rglob(\"*.pdf\")) if RECURSIVE else sorted(target.glob(\"*.pdf\"))\n",
        "pdfs = [p for p in pdfs if p.is_file()]\n",
        "print(f\"[INFO] Found PDFs: {len(pdfs)}\")\n",
        "\n",
        "rows = []\n",
        "\n",
        "for pdf_path in pdfs:\n",
        "    # 1) テキスト抽出\n",
        "    txt = extract_text_pages(pdf_path, PAGES_TEXT_SCAN)\n",
        "\n",
        "    # 2) DOI/arXiv抽出（テキスト）\n",
        "    doi = find_doi(txt)\n",
        "    arxiv_id = None if doi else find_arxiv_id(txt)\n",
        "\n",
        "    # 3) OCR（必要なときだけ）\n",
        "    ocr_txt = \"\"\n",
        "    if not doi and not arxiv_id:\n",
        "        ocr_txt = ocr_pages(pdf_path, PAGES_OCR_SCAN)\n",
        "        doi = find_doi(ocr_txt) or doi\n",
        "        arxiv_id = find_arxiv_id(ocr_txt) or arxiv_id\n",
        "\n",
        "    yyyymm = None\n",
        "    title = None\n",
        "    author = None\n",
        "    source = \"\"\n",
        "    title_sim = None\n",
        "\n",
        "    # 4) DOIがある → Crossref確定\n",
        "    if doi:\n",
        "        msg = crossref_lookup_by_doi(doi)\n",
        "        if msg:\n",
        "            yyyymm = pick_yyyymm_crossref(msg)\n",
        "            title  = pick_title_crossref(msg)\n",
        "            author = pick_first_author_crossref(msg)\n",
        "            source = f\"crossref:doi:{doi}\"\n",
        "\n",
        "    # 5) arXivがある → arXiv確定\n",
        "    if (not title or not author) and arxiv_id:\n",
        "        a = arxiv_lookup(arxiv_id)\n",
        "        if a and a.get(\"title\"):\n",
        "            title = title or a[\"title\"]\n",
        "            author = author or (a[\"authors\"][0] if a.get(\"authors\") else None)\n",
        "            yyyymm = yyyymm or a.get(\"yyyymm\")\n",
        "            source = source or f\"arxiv:{arxiv_id}\"\n",
        "\n",
        "    # 6) タイトル/著者をレイアウトから推定（フォントサイズ）\n",
        "    if not title or not author:\n",
        "        t2, a2 = extract_title_author_by_layout(pdf_path)\n",
        "        title = title or t2\n",
        "        author = author or a2\n",
        "\n",
        "    # 7) Crossrefで「タイトル→DOI逆引き」（最後の強手段）\n",
        "    if (not doi) and title:\n",
        "        guess = crossref_search_by_title(title, author=author, rows=7)\n",
        "        if guess:\n",
        "            doi = guess.get(\"DOI\") or doi\n",
        "            yyyymm = yyyymm or pick_yyyymm_crossref(guess)\n",
        "            title  = pick_title_crossref(guess) or title\n",
        "            author = pick_first_author_crossref(guess) or author\n",
        "            title_sim = guess.get(\"_title_similarity\")\n",
        "            source = source or f\"crossref:search(sim={title_sim})\"\n",
        "\n",
        "    # 8) 最終フォールバック\n",
        "    if not yyyymm:\n",
        "        yyyymm = \"UNKNOWN\"\n",
        "    if not title:\n",
        "        title = pdf_path.stem\n",
        "    if not author:\n",
        "        author = \"UNKNOWN\"\n",
        "\n",
        "    new_name = f\"{yyyymm}_{safe_component(author, MAX_AUTHOR_LEN)}_{safe_component(title, MAX_TITLE_LEN)}.pdf\"\n",
        "    dst = unique_path(pdf_path.with_name(new_name))\n",
        "\n",
        "    will_rename = (dst.name != pdf_path.name)\n",
        "\n",
        "    rows.append({\n",
        "        \"src\": str(pdf_path),\n",
        "        \"dst\": str(dst),\n",
        "        \"will_rename\": will_rename,\n",
        "        \"doi\": doi or \"\",\n",
        "        \"arxiv\": arxiv_id or \"\",\n",
        "        \"source\": source,\n",
        "        \"title_similarity\": title_sim if title_sim is not None else \"\"\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# 変更対応表CSVを必ず保存（元に戻せる）\n",
        "map_path = target / f\"rename_map_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "df.to_csv(map_path, index=False)\n",
        "print(f\"[INFO] Saved mapping CSV: {map_path}\")\n",
        "\n",
        "print(\"\\n=== Preview (first 40) ===\")\n",
        "display(df.head(40))\n",
        "\n",
        "if DRY_RUN:\n",
        "    print(\"\\n[DRY_RUN=True] まだリネームしていません。問題なければ DRY_RUN=False にして再実行してください。\")\n",
        "else:\n",
        "    changed = 0\n",
        "    for _, r in df.iterrows():\n",
        "        if not r[\"will_rename\"]:\n",
        "            continue\n",
        "        Path(r[\"src\"]).rename(Path(r[\"dst\"]))\n",
        "        changed += 1\n",
        "    print(f\"\\n[DONE] Renamed: {changed} files\")"
      ],
      "metadata": {
        "id": "Cq_YbZVuTQtS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}